# -*- coding: utf-8 -*-
"""PySpark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QThSJUoUDcjr_t5Q_Lsraf-Mmjk4s5yM

**Instalar sessão**
"""

# instalar as dependências
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz
!tar xf spark-2.4.4-bin-hadoop2.7.tgz
!pip install -q findspark

# configurar as variáveis de ambiente
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.4-bin-hadoop2.7"

# tornar o pyspark "importável"
import findspark
findspark.init('spark-2.4.4-bin-hadoop2.7')

# bibliotecas importantes
import pyspark.sql.functions as SF

"""# **Leitura**"""

# iniciar uma sessão local
from pyspark.sql import SparkSession
sc = SparkSession.builder.master('local[*]').getOrCreate()

# carregar dados de um csv
df_spark = sc.read.csv("/content/worldcities.csv")

# ver algumas informações sobre os tipos de dados de cada coluna
df_spark.printSchema()

df_spark.show()

"""Usando header para setar os nomes das colunas corretamente"""

df_spark1 = sc.read.csv("/content/worldcities.csv", header=True)

df_spark1.show()

df_spark1

"""Como poder ver acima, todas as colunas são do tipo string, vamos ajeitar usando inferSchema"""

df_spark2 = sc.read.csv("/content/worldcities.csv", inferSchema=True, header=True)

df_spark2

df_spark2.show()

df_spark2.printSchema()

type(df_spark2)

"""# **Leitura: Modo 2 (mais usado)**"""

df_spark = (sc
            .read
            .format("csv")
            .option("header", "true")
            .option("inferSchema", "true")
            .load("/content/worldcities.csv")
            )

print(df_spark)

df_spark.printSchema()

type(df_spark)

df_spark.show(3)

df_spark.head(3)

"""# **Select**"""

df_spark.select("city", "country", "population").show(5)

# mostra os comandos que podem ser realizados no df
dir(df_spark)

# mostra os comandos que podem ser realizados em uma coluna
dir(SF.col("city"))

"""**Fazendo uma seleção específica**"""

# - as duas linhas de código comentadas funcionam da mesma forma de como a ultima linha representa

#filtro_df = df_spark.select(SF.col("city"), SF.col("country"), SF.col("population")).filter(SF.col("population") < 10000)
#filtro_df = df_spark.select(SF.col("city"), SF.col("country"), SF.col("population")).filter("population < 10000")
filtro_df = df_spark.select(SF.col("city"), SF.col("country"), SF.col("population")).filter(df_spark.population < 10000)

filtro_df.show(5)

"""*Outra forma de realizar a operação acima*"""

colunas = ["city", "country", "population"]
filtro = SF.col("population") < 10000

df_spark.select(*colunas).filter(filtro).show(5)

"""**Aplicando mais de um filtro**"""

# selecionando todas as colunas dessa vez, então não vou usar o select
#df_spark.filter(df_spark.population < 10000).filter(df_spark.country == "India").show(5)
df_spark.filter((SF.col("population") < 10000) & (SF.col("country") == "India")).show(5) # pode-se usar where ao inves de filter

df_spark.filter((SF.col("population") < 10000) | (SF.col("country") == "India")).show(8) # pode-se usar where ao inves de filter

"""*Usando o comando "like" para filtrar*"""

#df_spark.filter("admin_name like 'Tai%'").show(5) - linguagem SQL
df_spark.filter(SF.col("admin_name").like("Tai%")).show(5)

df_spark.filter("capital in ('admin', 'primary')").show(5)

df_spark.filter(SF.col("country").startswith("F")).show(5)