# -*- coding: utf-8 -*-
"""PySpark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QThSJUoUDcjr_t5Q_Lsraf-Mmjk4s5yM

**Instalar sessão**
"""

# instalar as dependências
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz
!tar xf spark-2.4.4-bin-hadoop2.7.tgz
!pip install -q findspark

# configurar as variáveis de ambiente
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.4-bin-hadoop2.7"

# tornar o pyspark "importável"
import findspark
findspark.init('spark-2.4.4-bin-hadoop2.7')

# bibliotecas importantes
import pyspark.sql.functions as SF

"""# **Leitura**"""

# iniciar uma sessão local
from pyspark.sql import SparkSession
sc = SparkSession.builder.master('local[*]').getOrCreate()

# carregar dados de um csv
df_spark = sc.read.csv("/content/worldcities.csv")

# ver algumas informações sobre os tipos de dados de cada coluna
df_spark.printSchema()

df_spark.show()

"""Usando header para setar os nomes das colunas corretamente"""

df_spark1 = sc.read.csv("/content/worldcities.csv", header=True)

df_spark1.show()

df_spark1

"""Como poder ver acima, todas as colunas são do tipo string, vamos ajeitar usando inferSchema"""

df_spark2 = sc.read.csv("/content/worldcities.csv", inferSchema=True, header=True)

df_spark2

df_spark2.show()

df_spark2.printSchema()

type(df_spark2)

"""# **Leitura: Modo 2 (mais usado)**"""

df_spark = (sc
            .read
            .format("csv")
            .option("header", "true")
            .option("inferSchema", "true")
            .load("/content/worldcities.csv")
            )

print(df_spark)

df_spark.printSchema()

type(df_spark)

df_spark.show(3)

df_spark.head(3)

"""# **Schemas**

*É uma opção para não precisar usar cast ou para diretamente criar um schema setando os tipos das variaveis logo no carregamento*
"""

from pyspark.sql.types import StructType, StructField, StringType, IntegerType

sc = SparkSession.builder.master('local[*]').getOrCreate()

schema = (StructType([
                      StructField("city", StringType(), True),
                      StructField("city_ascii", StringType(), True),
                      StructField("lat", IntegerType(), True),
                      StructField("lng", IntegerType(), True),
                      StructField("country", StringType(), True),
                      StructField("iso2", StringType(), True),
                      StructField("iso3", StringType(), True),
                      StructField("admin_name", StringType(), True), 
                      StructField("capital", StringType(), True), 
                      StructField("population", IntegerType(), True), 
                      StructField("id", IntegerType(), True),              
])
)

df_spark100 = (sc
            .read
            .format("csv")
            .option("header", "true")
            .option("inferSchema", "false") # perceba o inferSchema aqui = False
            .load("/content/worldcities.csv", schema = schema))

# Perceba os resultados do output, eles foram setados da forma como foi configurado no schema
df_spark100.printSchema()

"""# **Select**"""

df_spark.select("city", "country", "population").show(5)

# mostra os comandos que podem ser realizados no df
dir(df_spark)

# mostra os comandos que podem ser realizados em uma coluna
dir(SF.col("city"))

"""**Fazendo uma seleção específica**"""

# - as duas linhas de código comentadas funcionam da mesma forma de como a ultima linha representa

#filtro_df = df_spark.select(SF.col("city"), SF.col("country"), SF.col("population")).filter(SF.col("population") < 10000)
#filtro_df = df_spark.select(SF.col("city"), SF.col("country"), SF.col("population")).filter("population < 10000")
filtro_df = df_spark.select(SF.col("city"), SF.col("country"), SF.col("population")).filter(df_spark.population < 10000)

filtro_df.show(5)

"""*Outra forma de realizar a operação acima*"""

colunas = ["city", "country", "population"]
filtro = SF.col("population") < 10000

df_spark.select(*colunas).filter(filtro).show(5)

"""**Aplicando mais de um filtro**"""

# selecionando todas as colunas dessa vez, então não vou usar o select
#df_spark.filter(df_spark.population < 10000).filter(df_spark.country == "India").show(5)
df_spark.filter((SF.col("population") < 10000) & (SF.col("country") == "India")).show(5) # pode-se usar where ao inves de filter

df_spark.filter((SF.col("population") < 10000) | (SF.col("country") == "India")).show(8) # pode-se usar where ao inves de filter

"""*Usando o comando "like" para filtrar*"""

#df_spark.filter("admin_name like 'Tai%'").show(5) - linguagem SQL
df_spark.filter(SF.col("admin_name").like("Tai%")).show(5)

df_spark.filter("capital in ('admin', 'primary')").show(5)

df_spark.filter(SF.col("country").startswith("F")).show(5)

"""# **Operações entre colunas do DF**"""

# o método cast() faz com que vc mude o tipo da coluna, i.e: integer para string 
df_spark_cast = df_spark.select(SF.col("lat").cast("string"), SF.col("lng").cast("string"))

df_spark_cast.printSchema()

df_spark.printSchema()

# método withColumn cria uma nova coluna
# método lit cria valores de string
# método concat concantena
df_spark_coluna_nova = df_spark.withColumn("coordinates", SF.concat(SF.col("lat"), SF.lit(", "), SF.col("lng")))

df_spark_coluna_nova.show(5)

# método when em conjunto com otherwise cria opções para gerar uma coluna baseada em valores
df_spark_coluna_nova = df_spark_coluna_nova.withColumn("localization", SF.when(SF.col("lat") > 10, "Hemisferio Norte").otherwise("Hemisferio Sul"))

df_spark_coluna_nova.show(5)

"""**Coletar porções (substring) de certos valores em uma coluna**"""

# é um bom método para separar datas em dia, mes e ano
df_substring = df_spark.withColumn("Primeiras letras", SF.substring(SF.col("city"), 1, 3))

df_substring.show(5)